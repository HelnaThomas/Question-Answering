{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0SeyxYVU3Ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1177f68-59a7-47ae-9786-5ccaab07b123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/645.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/645.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m645.1/645.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.3/645.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain==0.0.149 pyPDF2 faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voxHBQIhWYHF",
        "outputId": "0e3f3e5a-7fe9-45aa-829f-15a4925f2e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.149\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://www.github.com/hwchase17/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity, tqdm\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "doc_readr=PdfReader('/impromptu-rh.pdf')"
      ],
      "metadata": {
        "id": "FJk2JxFGWYSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_readr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_ReBjgBYchT",
        "outputId": "e1fc22ac-1e11-49e3-9d6c-434c263906d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PyPDF2._reader.PdfReader at 0x7f3c2c1da620>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read data from the file and put them into a variable called raw_text\n",
        "raw_text = ''\n",
        "for i, page in enumerate(doc_readr.pages):\n",
        "    text = page.extract_text()\n",
        "    if text:\n",
        "        raw_text += text"
      ],
      "metadata": {
        "id": "CCkTcxz8WYc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting up the text into smaller chunks for indexing\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    #separator = \"\\n\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200, #striding over the text\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "UQEWCc-9bIXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Huggingface embedding\n",
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "embeddings = HuggingFaceHubEmbeddings(\n",
        "                repo_id=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "                task=\"feature-extraction\",\n",
        "                huggingfacehub_api_token=\"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\",)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77cpvQjYsAip",
        "outputId": "4c7f7342-e998-413e-dc9f-1f1df4d1d633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n",
            "WARNING:huggingface_hub.inference_api:You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = embeddings.embed_documents(\n",
        "    [\n",
        "        \"Hi there!\",\n",
        "        \"Oh, hello!\",\n",
        "        \"What's your name?\",\n",
        "        \"My friends call me World\",\n",
        "        \"Hello World!\"\n",
        "    ]\n",
        ")\n",
        "len(vectors), len(vectors[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElZ43kmzsFvR",
        "outputId": "0c5a2ad6-d417-487d-c681-935f9a40fff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_query = embeddings.embed_query(\"What was the name mentioned in the conversation?\")\n",
        "embedded_query\n",
        "!pip install cohere\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "embeddings = CohereEmbeddings(\n",
        "                model=\"embed-english-light-v2.0\", cohere_api_key=\"6k408g3yPCEhAZGpdq4RlO0Dd961ouv1ibC5ZVwH\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01l-X76UsKvA",
        "outputId": "e6ddc57c-6b68-44ef-8dd5-98463bfbf905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-4.45-py3-none-any.whl (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.3)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
            "  Downloading fastavro-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib_metadata<7.0,>=6.0 (from cohere)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.11.17)\n",
            "Installing collected packages: importlib_metadata, fastavro, backoff, cohere\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 cohere-4.45 fastavro-1.9.3 importlib_metadata-6.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "embeddings = CohereEmbeddings(\n",
        "                model=\"embed-english-light-v2.0\", cohere_api_key=\"6k408g3yPCEhAZGpdq4RlO0Dd961ouv1ibC5ZVwH\"\n",
        "            )\n",
        "!pip install tensorflow_text\n",
        "from langchain.embeddings import TensorflowHubEmbeddings\n",
        "url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
        "embeddings = TensorflowHubEmbeddings(model_url=url)"
      ],
      "metadata": {
        "id": "C-U66Kl5sTYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "docsearch = FAISS.from_texts(texts[:250],embeddings)"
      ],
      "metadata": {
        "id": "2TDQPYtGsd0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how does GPT-4 change social media?\" #how much data producing daily?  (page 84)\n",
        "docs = docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "uw1-bWHLsuUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "model = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", huggingfacehub_api_token=\"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\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlVU5bXksvzJ",
        "outputId": "725b8632-6be9-4805-ebd7-831e7d8a7e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "chain = load_qa_chain(model,\n",
        "                      chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "2-xUuQjhs6kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how much data producing daily?\"   #how much data producing daily?  (page 84)\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs,question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "LYMqMkgqtBZQ",
        "outputId": "abe17ed1-419a-4026-829b-e216c1f585d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' According to a report by the International Data Corporation (IDC), the average daily data creation in 2022 is approximately 375 billion gigabytes. This represents a significant increase from previous years due to the exponential growth of data production. For context, one exabyte of data is enough to store about 250 million DVDs or to stream Netflix for eighty-six million years. This data deluge presents both a challenge and an opportunity for the journal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who is the book authored by?\"   #By Reid Hoffman\n",
        "docs = docsearch.similarity_search(query,k=4)\n",
        "chain.run(input_documents=docs,question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1mDPPT9qtG3H",
        "outputId": "a1022959-df2b-4ce5-de95-38cb853acd2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The book is authored by Reid Hoffman and Ben Casnocha, with contributions from GPT-4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(model,\n",
        "                      chain_type=\"map_reduce\")"
      ],
      "metadata": {
        "id": "rl2JA6hStN1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(model,\n",
        "                      chain_type=\"map_rerank\",\n",
        "                      return_intermediate_steps=True\n",
        "      )"
      ],
      "metadata": {
        "id": "4gPj3sgUtRIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# set up FAISS as a generic retriever\n",
        "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
        "\n",
        "# create the chain to answer questions\n",
        "rqa = RetrievalQA.from_chain_type(llm=model,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "yf0MgQI_tVIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rqa(\"What is OpenAI?\")"
      ],
      "metadata": {
        "id": "-PB-XepitYm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3HoJoO8gteaf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}